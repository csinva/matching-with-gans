{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cached labels\n",
      "reload latents...\n",
      "calculating gan dists...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from os.path import join as oj\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from matching import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data\n",
    "import util\n",
    "\n",
    "DIR_ORIG = '../data/celeba-hq/ims/'\n",
    "DIR_GEN = oj('../data_processed/celeba-hq/generated_images_0.1')\n",
    "\n",
    "\n",
    "# df contains filenames, ids, and attributes\n",
    "df = data.load_all_labs()\n",
    "df = df.set_index('fname_id')\n",
    "df = df[['id', 'fname_final', 'age', 'gender', 'race_pred', 'race4_pred', 'Eyeglasses']] # only keep few keys to speed things up\n",
    "\n",
    "# get fnames\n",
    "fname_nps = [f for f in sorted(os.listdir(DIR_GEN)) if 'npy' in f] # these start at 00001\n",
    "fnames_calculated = np.array([f[:-4] for f in fname_nps]) # these start at 00001\n",
    "idxs_calculated = np.array([int(x) - 1 for x in fnames_calculated]) # this starts at 0\n",
    "df = df.loc[fnames_calculated]\n",
    "\n",
    "# load latents and calculate dists\n",
    "print('reload latents...')\n",
    "latents = np.array([np.load(oj(DIR_GEN, f)) for f in fname_nps])\n",
    "lats = get_lat(latents)\n",
    "preds = np.ones((lats.shape[0], 6)) #lats @ coefs.T + intercepts.T\n",
    "weights = np.ones(preds.shape[1])\n",
    "# print(ATTR_TO_INDEX)\n",
    "# weights[ATTR_TO_INDEX['skin-color']] = 1e2\n",
    "vecs = join_vecs(preds, lats, weights)\n",
    "print('calculating gan dists...')\n",
    "dists_gan = get_dists(vecs)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load facial dists\n",
      "facial dists loaded\n"
     ]
    }
   ],
   "source": [
    "# load pairwise facial dicts\n",
    "print('load facial dists')\n",
    "dists_facial = np.load(open('processed/13_facial_dists_pairwise.npy', 'rb')) # pkl.load()\n",
    "dists_facial = dists_facial[idxs_calculated]\n",
    "dists_facial = dists_facial[:, idxs_calculated]\n",
    "print('facial dists loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Race=black'] = (df['race_pred'] == 'Black').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find best matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we want to find matched pairs\n",
    "the pairs should change only one attribute at a time (but we may vary 2 to make a transect)\n",
    "\"main\" image is matched stringently\n",
    "\"reference\" image is matched more loosely\n",
    "'''\n",
    "\n",
    "\n",
    "# specify dists for matching\n",
    "dists_match_name = 'facial'\n",
    "if dists_match_name == 'facial':\n",
    "    dists_match = dists_facial\n",
    "elif dists_match_name == 'gan':\n",
    "    dists_match = dists_gan\n",
    "elif dists_amtch_name == 'gan_constrained':\n",
    "    dists_match = dists_gan + (dists_facial > 0.6) * 1e3 # constraint for missclassificaiton\n",
    "\n",
    "dists_ref = dists_gan\n",
    "n = dists_match.shape[0]\n",
    "assert df.shape[0] == n, 'df shapes must match'\n",
    "assert dists_ref.shape[0] == n, 'ref shapes must match'\n",
    "\n",
    "# specify things to vary (these should all be binary columns)\n",
    "# id willl automatically be different when we vary gender, race bc these things \n",
    "# are forced to be preserved in data.py\n",
    "attrs_to_vary = ['gender'] #Race=black'] # Eyeglasses, gender, 'Race=black'\n",
    "\n",
    "# specify constraint for reference\n",
    "def apply_reference_constraints(d):\n",
    "    REFERENCE_CONSTRAINTS = {\n",
    "        'Eyeglasses': 0, # default should be no Eyeglasses\n",
    "    }\n",
    "    for k in REFERENCE_CONSTRAINTS.keys():\n",
    "        d = d[d[k] == REFERENCE_CONSTRAINTS[k]]\n",
    "    return d\n",
    "\n",
    "## select only ims with a reference image satisfying reference constraints\n",
    "def get_idxs_satisfying_reference_constraints(df, n, dists_match, dists_ref):\n",
    "    idxs_orig = pd.Series(1, df.index) #df['bool'] # np.ones(n).astype(bool)\n",
    "    \n",
    "    # prune anything for which dists are constant (the code for NaN)\n",
    "    for i in tqdm(range(n)):\n",
    "        if np.all(dists_match[i] == dists_match[i, 0]) or np.all(dists_ref[i] == dists_ref[i, 0]):\n",
    "            idxs_orig.iloc[i] = False\n",
    "    \n",
    "    # prune things based on reference constraints\n",
    "    for i in tqdm(sorted(df.id.unique())):\n",
    "        d = df[(df.id == i) & idxs_orig]\n",
    "\n",
    "        # if there is only one photo for this id, don't pick it\n",
    "        if d.shape[0] == 1:\n",
    "            idxs_orig.loc[d.index] = False\n",
    "        \n",
    "        else:\n",
    "            # look for valid ref photos\n",
    "            d_ref = apply_reference_constraints(d)\n",
    "\n",
    "            # if there is no valid ref, don't pick any image with this id\n",
    "            if d_ref.shape[0] < 1:\n",
    "                idxs_orig.loc[d.index] = False\n",
    "\n",
    "            # if there is 1 valid ref, don't pick the reference image\n",
    "            # (we can still pick a different photo with this id)\n",
    "            elif d_ref.shape[0] == 1:\n",
    "                idxs_orig.loc[d_ref.index] = False \n",
    "                \n",
    "    return idxs_orig.values.astype(bool)\n",
    "\n",
    "idxs_orig = get_idxs_satisfying_reference_constraints(df, n, dists_match, dists_ref)\n",
    "print('total ims', n, 'selectable ims', np.sum(idxs_orig))\n",
    "print('total ids', df.id.unique().size, 'selectable ids', df[idxs_orig].id.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MATCHES = 1000\n",
    "# find allowed indices for each group\n",
    "subgroups = {}\n",
    "for a in attrs_to_vary:\n",
    "    for val in [0, 1]:\n",
    "        subgroups[f'{a}_{val}'] = (df[a].values == val) & idxs_orig\n",
    "\n",
    "matches = {}\n",
    "matches_skipped = []\n",
    "pairwise_constraints = np.zeros((n, n)).astype(bool) # extra constraints for matching\n",
    "\n",
    "\n",
    "\n",
    "for match_num in tqdm(range(NUM_MATCHES)):\n",
    "    # loop to create best matches\n",
    "    for i, a in enumerate(attrs_to_vary):\n",
    "        # this should go in order from smaller groups -> bigger groups\n",
    "    #     vals = np.argsort([np.sum(subgroups[f'{a}_{val}']) for val in [0, 1]])\n",
    "    #     for j, val in enumerate([0, 1]):\n",
    "\n",
    "        ## find best match subject to constraints\n",
    "        s0 = f'{a}_{0}'\n",
    "        s1 = f'{a}_{1}'\n",
    "        idxs0 = subgroups[s0]\n",
    "        idxs1 = subgroups[s1]\n",
    "        C = np.sum(idxs1)\n",
    "        \n",
    "        # if there are no more possible matches, stop\n",
    "        if np.sum(idxs0) == 0 or np.sum(idxs1) == 0:\n",
    "            NUM_MATCHES = len(matches[s0])\n",
    "            break\n",
    "        \n",
    "        # extra constraints (e.g. previously skipped, could enforce that attributes are the same)\n",
    "        dists_match_constrained = deepcopy(dists_match)\n",
    "        dists_match_constrained[pairwise_constraints] = 1e5\n",
    "        \n",
    "        # main constraints (e.g. attributes are different, previously selected)\n",
    "        dists_constrained = dists_match_constrained[idxs0][:, idxs1] # (R, C)\n",
    "        \n",
    "        arg = np.argmin(dists_constrained)\n",
    "\n",
    "        # convert match arg back to space without constraints\n",
    "        arg0 = arg // C\n",
    "        arg1 = arg % C\n",
    "        idx0 = np.where(idxs0)[0][arg0]\n",
    "        idx1 = np.where(idxs1)[0][arg1]\n",
    "        \n",
    "        ## find reference images\n",
    "        idxs_ref = pd.Series(True, df.index)\n",
    "        id0 = df.iloc[idx0].id\n",
    "        id1 = df.iloc[idx1].id\n",
    "        idxs_ref0 = idxs_ref & (df.id == id0) & (np.arange(n) != idx0)\n",
    "        idxs_ref1 = idxs_ref & (df.id == id1) & (np.arange(n) != idx1)\n",
    "        assert np.sum(idxs_ref0) > 0 and np.sum(idxs_ref1) > 0, 'must have valid reference images'\n",
    "        \n",
    "        ## compare dists for reference images\n",
    "        dists0 = dists_ref[idx0][idxs_ref0].reshape(-1, 1)\n",
    "        dists1 = dists_ref[idx1][idxs_ref1].reshape(-1, 1)\n",
    "        \n",
    "        ## pairwise distances\n",
    "        MIN_REF_DIST_THRESH_UPPER = 1e2 # 1 will weed out any crazy matches\n",
    "        MIN_REF_DIST_THRESH_LOWER = 1e-2 # 1 will weed out any crazy matches\n",
    "        dists_ref_dists = sklearn.metrics.pairwise_distances(dists0, dists1, metric='l1')\n",
    "        C_ref = dists1.size\n",
    "        arg_ref = np.argmin(dists_ref_dists)\n",
    "        min_ref_dist = np.min(dists_ref_dists)\n",
    "        \n",
    "        # if no good reference match, then skip for now (might come back later)\n",
    "        if min_ref_dist > MIN_REF_DIST_THRESH_UPPER \\\n",
    "        or min_ref_dist < MIN_REF_DIST_THRESH_LOWER: # match too similar (e.g. duplicate image)\n",
    "            matches_skipped.append({\n",
    "                'idx0': idx0,\n",
    "                'idx1': idx1,\n",
    "                'dist': min_ref_dist,\n",
    "            })\n",
    "            pairwise_constraints[idx0, idx1] = True\n",
    "        else:\n",
    "            ## pick best match\n",
    "            arg_ref0 = arg_ref // C_ref\n",
    "            arg_ref1 = arg_ref % C_ref\n",
    "            idx_ref0 = np.where(idxs_ref0)[0][arg_ref0]\n",
    "            idx_ref1 = np.where(idxs_ref1)[0][arg_ref1]\n",
    "            \n",
    "            ## save the pairs\n",
    "            match = {\n",
    "                s0: idx0,\n",
    "                s1: idx1,\n",
    "                s0 + '_ref': idx_ref0,\n",
    "                s1 + '_ref': idx_ref1,\n",
    "                'dist': dists_constrained[arg0, arg1],\n",
    "                'dist_ref0': dists0[arg_ref0][0],\n",
    "                'dist_ref1': dists1[arg_ref1][0],\n",
    "            }\n",
    "            for k in match.keys():\n",
    "                if not k in matches:\n",
    "                    matches[k] = []\n",
    "                matches[k].append(match[k])\n",
    "\n",
    "            ## remove them from further consideration\n",
    "            idxs_to_remove = (df.id == id0) | (df.id == id1)\n",
    "            subgroups[s0][idxs_to_remove] = False\n",
    "            subgroups[s1][idxs_to_remove] = False\n",
    "#         print(matches, matches_skipped)\n",
    "matches = pd.DataFrame.from_dict(matches).infer_objects()\n",
    "matches.to_pickle(f'processed/14_matches_{attrs_to_vary[0]}_{matches.shape[0]}_{dists_match_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_pickle(f'processed/14_matches_{attrs_to_vary[0]}_{matches.shape[0]}')\n",
    "def get_im_from_match_key(k, match):\n",
    "    d = df.iloc[int(match[k])]\n",
    "    return mpimg.imread(oj(DIR_ORIG, d.fname_final))\n",
    "\n",
    "for a in attrs_to_vary:\n",
    "    ks = [f'{a}_0_ref', f'{a}_0', f'{a}_1', f'{a}_1_ref']\n",
    "    r = {\n",
    "        k: [] for k in ks\n",
    "    }\n",
    "    dist_keys = {\n",
    "        f'{a}_0': 'dist',\n",
    "        f'{a}_1': 'dist',\n",
    "        f'{a}_0_ref': 'dist_ref0',\n",
    "        f'{a}_1_ref': 'dist_ref1'\n",
    "    }\n",
    "\n",
    "    for i, match in tqdm(matches.iterrows()):\n",
    "        for k in r.keys():\n",
    "            r[k].append(get_im_from_match_key(k, match))\n",
    "\n",
    "    kwargs = {\n",
    "        'dpi': 150\n",
    "    }\n",
    "    for i, k in enumerate(ks):\n",
    "        util.plot_row(r[k], annot_list=matches[dist_keys[k]].round(1),\n",
    "                      ylab=k.capitalize())\n",
    "        plt.savefig(f'results/{k}.png', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_0</th>\n",
       "      <th>gender_1</th>\n",
       "      <th>gender_0_ref</th>\n",
       "      <th>gender_1_ref</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_ref0</th>\n",
       "      <th>dist_ref1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16503</td>\n",
       "      <td>10229</td>\n",
       "      <td>7876</td>\n",
       "      <td>11784</td>\n",
       "      <td>98.712203</td>\n",
       "      <td>113.304102</td>\n",
       "      <td>113.406888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1895</td>\n",
       "      <td>3902</td>\n",
       "      <td>1642</td>\n",
       "      <td>10100</td>\n",
       "      <td>99.306843</td>\n",
       "      <td>111.913963</td>\n",
       "      <td>111.655861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9216</td>\n",
       "      <td>9404</td>\n",
       "      <td>8076</td>\n",
       "      <td>10568</td>\n",
       "      <td>99.347743</td>\n",
       "      <td>119.696971</td>\n",
       "      <td>119.642936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>10655</td>\n",
       "      <td>9778</td>\n",
       "      <td>5580</td>\n",
       "      <td>99.575084</td>\n",
       "      <td>117.101523</td>\n",
       "      <td>120.145386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>5100</td>\n",
       "      <td>3868</td>\n",
       "      <td>9320</td>\n",
       "      <td>100.023403</td>\n",
       "      <td>120.401611</td>\n",
       "      <td>121.312881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1370</td>\n",
       "      <td>6556</td>\n",
       "      <td>4828</td>\n",
       "      <td>612</td>\n",
       "      <td>107.610354</td>\n",
       "      <td>115.528533</td>\n",
       "      <td>116.199096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10848</td>\n",
       "      <td>14298</td>\n",
       "      <td>10782</td>\n",
       "      <td>13179</td>\n",
       "      <td>107.675036</td>\n",
       "      <td>93.285602</td>\n",
       "      <td>111.150179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>185</td>\n",
       "      <td>1890</td>\n",
       "      <td>12883</td>\n",
       "      <td>4188</td>\n",
       "      <td>107.675125</td>\n",
       "      <td>124.426500</td>\n",
       "      <td>80.849539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8668</td>\n",
       "      <td>8208</td>\n",
       "      <td>12256</td>\n",
       "      <td>15934</td>\n",
       "      <td>107.790840</td>\n",
       "      <td>119.889391</td>\n",
       "      <td>115.886389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>8242</td>\n",
       "      <td>8385</td>\n",
       "      <td>8106</td>\n",
       "      <td>9</td>\n",
       "      <td>107.832331</td>\n",
       "      <td>134.198821</td>\n",
       "      <td>133.932239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender_0  gender_1  gender_0_ref  gender_1_ref        dist   dist_ref0  \\\n",
       "0      16503     10229          7876         11784   98.712203  113.304102   \n",
       "1       1895      3902          1642         10100   99.306843  111.913963   \n",
       "2       9216      9404          8076         10568   99.347743  119.696971   \n",
       "3        107     10655          9778          5580   99.575084  117.101523   \n",
       "4       1465      5100          3868          9320  100.023403  120.401611   \n",
       "..       ...       ...           ...           ...         ...         ...   \n",
       "92      1370      6556          4828           612  107.610354  115.528533   \n",
       "93     10848     14298         10782         13179  107.675036   93.285602   \n",
       "94       185      1890         12883          4188  107.675125  124.426500   \n",
       "95      8668      8208         12256         15934  107.790840  119.889391   \n",
       "96      8242      8385          8106             9  107.832331  134.198821   \n",
       "\n",
       "     dist_ref1  \n",
       "0   113.406888  \n",
       "1   111.655861  \n",
       "2   119.642936  \n",
       "3   120.145386  \n",
       "4   121.312881  \n",
       "..         ...  \n",
       "92  116.199096  \n",
       "93  111.150179  \n",
       "94   80.849539  \n",
       "95  115.886389  \n",
       "96  133.932239  \n",
       "\n",
       "[97 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}